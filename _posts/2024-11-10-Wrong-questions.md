---
title: 'Asking the right questions'
description: ""
date: 2024-6-28
permalink: /posts/2024/6/The-strange-case-of-the-emerging-dead-journal/
tags:
  - AI
  - epistemology
  - philosophical questions
---

The landscape of Italian journalism is quite depressing, from my point of view. I mostly listen to an independent local radio channel ([Radio Popolare](https://www.radiopopolare.it/) - by the way, they anticipated crowdfunding with a subscription mechanism although this subscription does not imply that they produce contents accessible only to subscribers, a curious and interesting approach) and read [Internazionale](https://www.internazionale.it/), a magazine collecting and translating in Italian articles from International journals from all over the world. The last number of Internazionale proposed the translation in Italian of [this article](https://thewalrus.ca/ai-hype/) titled "AI Is a False God - The real threat with super intelligence is falling prey to the hype", by Navneet Alang, from The Walrus, another editorial initiative that is worth following.

I am not completely sure about what I think about the article: I appreciate the reference to Morozov and an anti pan-optimistic perspective on the implications of technological development. However, since it's from a person whose cultural background is quite different from mine, and since it deals with topics I don't have a deep knowledge of, but that I find particularly important, I have the decency to avoid trying to summarize it. I will, though, report a quote from the article that connected with some intuitions I had considering the recent trends in AI literature and some experiences from attending [ECAI 2024](https://www.ecai2024.eu/). Here's the quote:

>If you find yourself asking AI about the meaning of life, it isn’t the answer that’s wrong. It’s the question.

I don't want to talk about topics that, again, are out of the reach of my knowledge and competencies, like the question if [AI could become a conscious, sentient being](https://arstechnica.com/science/2024/07/could-ais-become-conscious-right-now-we-have-no-way-to-tell/) or (like in Navneet Alang's article) if AI could be a "shortcut to objectivity or ultimate meaning" (if such a thing could even exist outside Douglas Adams’ book or a Prolog interpreter).

What really struck a nerve was the "wrong question" expression. As a researcher, I think it's very important to periodically ask myself  **"Am I asking the right (research) questions?"**.

![Rodin's 'The Thinker' by Edvard Munch](https://upload.wikimedia.org/wikipedia/commons/4/49/Edvard_Munch%2C_Le_Penseur_de_Rodin_dans_le_parc_du_Docteur_Linde_%C3%A0_L%C3%BCbeck%2C_1907_.jpg)
*Rodin's 'The Thinker' by Edvard Munch (thanks, Wikipedia, it's nice to find out this connection between thinking and one of the most universally recognized representations of anxiety!)*

I have always been a *modeler*. I always tried to come up with ideas about models that either mimic some form of decision making activity or that can plausibly generate, as implications of some internal mechanism regulating portions of a composite system, an overall observable behavior that is object of study. Models I came up with were interesting because they could, to a certain extent, be used as a proxy of a reality, to support some form of analysis or decision making activity. The models I usually built were largely *manually defined*, based on some understanding of the system object of study, coming from relevant literature. I generally tried to improve my knowledge about some reality to make better models, and sometimes models helped me understand better that reality. Some [colleagues also more formally defined the concept of a "learning-by-modeling" approach](https://www.tandfonline.com/doi/abs/10.1080/01969722.2011.610266).

Of course, this process has deep relationships with data, with empirical evidences, but they were used to inspire creative human work on models (either new or modified/integrated to better reflect some evidence), not to substitute it by actually *learning* a model.

Don't get me wrong, it makes sense to try doing that, to investigate to which extent it is possible and what are the limits to this approach. I also started experimenting this kind of approach, learning decision making models for agents instead of defining them manually. I am not so sure this will be as useful to learn new things myself about the object of investigation, although I did start to learn something about machine learning.

Recently I have stumbled upon researches about the application of Large Language Models (LLM) for managing Non-Playing Characters in interactive systems (such as video games, Virtual Reality (VR) applications, and such), or some other application in which, basically, the LLM acts as an oracle, solving some problem of a connected system. I often asked myself: *what's the value of this research? What's the complexity of this research?* Well, in a sense, the work necessary to perform this kind of study is probably not that much, it basically leverages the work carried out to conceive, construct and train an LLM. However, despite the fact that maybe our work as researchers is not much, we can achieve an advancement of our knowledge as an outcome of this kind of work, if it is not carried out simplistically. LLMs are complex instruments, one could legitimately think that, as of this moment, some of the raised expectations will not be reached, but LLMs and their actual capabilities are a more than legitimate object of investigation. The question "can an LLM be used to carry out this task", maybe adding "and how does this approach compare to alternatives ones", generally makes sense, as long the research is carried out properly. In some time, I suspect we will see fewer researches like these, when we will have a better understanding of what can we do with LLMs, with which results, if and when it is convenient to use them instead of alternative, and typically more focused approaches. It will take time and honest, non biased, research.


I suspect that, sometimes, the feeling that it is a somewhat lesser kind of research is due to the fact that we don't see the *how* the LLM solves the problem, we don't really learn much about solving the problem, we delegate the solution to an oracle.

Whatever is your point of view on this topic, I hope you agree that it is a good thing to stop and, maybe without necessarily taking the theatrical pose of Rodin's statue, take the time to ask ourselves: "Am I asking the right (research) question?"
