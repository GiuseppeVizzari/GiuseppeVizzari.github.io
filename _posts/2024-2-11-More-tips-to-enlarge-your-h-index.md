---
title: 'More tips to enlarge your H-index'
description: "A couple of more tips to inflate your some bibliometric indicators..."
date: 2024-2-11
permalink: /posts/2024/2/More-tips-to-enlarge-your-h-index/
tags:
  - bibliometrics
  - gaming
  - research evaluation
---

In [an earlier post](https://giuseppevizzari.github.io/posts/2023/11/Gaming-101/) I started talking about bibliometrics, some of its flaws, and what I think has become one of the facets of an ideology, something Jerry Z. Muller called "The tyranny of metrics".I'm going to add two ways to _enlarge your H-index_ that, at the same time, also represent a totally legitimate action in the overall context of a line research.

First of all, I think I don't need to stress the importance of publicly available datasets, in the most different areas of research. In particular, I think that (together with the growth of computational power, in particular the diffusion of GPUs), the collective creation of datasets such as [ImageNet](https://www.image-net.org/) was instrumental in the growth of the deep learning techniques. It is interesting to note that ImageNet is:

1. a collective effort, that would not have been practically possible without the Internet;
1. organized according to [WordNet](https://wordnet.princeton.edu/), a _lexical resource_, not exactly an ontology (or knowledge graph), but certainly something closer to symbolic AI or [GOFAI](https://www.cambridge.org/core/books/abs/cambridge-handbook-of-artificial-intelligence/gofai/FCF7D6DD921658FE8AE9F2A2B0FECBDD), if you prefer.

Creating and sharing a dataset is therefore an undoubtedly good action, worth acknowledgement by the research community. Moreover it often makes a lot of sense to describe the dataset in a paper, not necessarily presenting any particular advancement in the state of the art or additional content besides the dataset per se, its structure and maybe the details of how the data was gathered. Sometimes gather new data involves scientific advancement, sometimes it doesn't, it's just drudgery (and in fact sometimes it paid - or underpaid - by means of tools like [Amazon's Mechanical Turk](https://www.mturk.com/)). The point is that anyone using the dataset is gently asked to cite the paper, and this is also fine with me, from an ethical perspective. Once again, the problem is the __interpretation of those citations__, that end up inflating metrics such as the H-index of the authors of the paper describing the dataset.

Someone, especially some colleagues in Italy that strenuously defend bibliometrics as a way to evaluate research quality even of individuals, might say "well, let's discount these citations, or let's evaluate dataset papers in a different way" (in a past research evaluation campaign in Italy self-declared review papers were evaluated differently, due to the obvious tendency to gather a higher number of citations, but it's more complicated to discount citations from structured indicators like H-index). It's not that simple, though, since sometimes the dataset is described in a paper showing its first usage for innovative research, improving our knowledge on some subject. Eventually, __papers should be read__ and evaluated according to their content, not the venue of publication nor the simple number of citations.

Another kind of paper that has appeared in the last years that brings up additional issues with reference to indiscriminate usage of citation number as an objective indicator of research quality is represented by challenge papers. Challenges represent totally positive research activities, and they also represent very reasonable ways to promote reproducibility of results, as well as a way to involve members of a research community in a generally healthy form of competition. Moreover, organizing a challenge represents a complicated matter, very often technical issues must be faced and solved to create shared resources enabling participants to share a common basic environment, and ways to produce results compliant to the same rules, and providing the same measurements. It's a hell of a work, and it deserves to be acknowledged, not unlike datasets. Again, the problem is the __indiscriminate usage of the citations to these papers as research quality indicators__.

I want to stress that, so far, I did not even introduce malicious tricks devised to distort bibliometric indicators, but __completely legitimate and even virtuous actions__ by researchers. Of course, one could think that sometimes the choice of doing these action could be influenced by the foreseen reward, but that would represent yet another example of usage of an expression often used by an old Italian politician (["a pensare male degli altri si fa peccato ma spesso ci si indovina" - Giulio Andreotti](https://it.wikiquote.org/wiki/Giulio_Andreotti)).
